{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.feature_selection import RFE, GenericUnivariateSelect, chi2, mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, RepeatedKFold\n",
    "\n",
    "from neurocombat_sklearn import CombatModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tools import *\n",
    "\n",
    "# Defining local variables\n",
    "data_path = 'C:/Users/b_charmettant/data/parotide_ml/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 with 432 features\n",
      "Labels : 58 benign lesions and 49 malignant\n",
      "Shape features : (107, 432)\n",
      "Shape meta variables : (107, 3)\n"
     ]
    }
   ],
   "source": [
    "df_meta = get_meta_data(data_path)\n",
    "\n",
    "type_to_include=['t1', 't2', 'gado', 'diff']\n",
    "# type_to_include=['t1']\n",
    "\n",
    "\n",
    "ls_exams, id_to_feat, feat_to_id = load_features(df_meta, data_path, verbose=False, type_to_include=type_to_include)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Meta data fields to include in meta variables for harmonization\n",
    "\n",
    "meta_fields = ['sexe', 'tesla', 'age']\n",
    "meta_variables = []\n",
    "\n",
    "for exam in ls_exams:\n",
    "    lbl, ft, meta = format_exam(exam, feat_to_id, meta_fields=meta_fields)\n",
    "    features.append(ft)\n",
    "    labels.append(lbl)\n",
    "    meta_variables.append(meta)\n",
    "    \n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "meta_variables = np.array(meta_variables)\n",
    "\n",
    "print(f\"{len(labels)} with {len(features[0])} features\")\n",
    "print(f\"Labels : {len(labels) - sum(labels)} benign lesions and {sum(labels)} malignant\")\n",
    "\n",
    "# Adjusting the scales of the different features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rescaled_features = scaler.fit_transform(features)\n",
    "\n",
    "print(f\"Shape features : {rescaled_features.shape}\")\n",
    "print(f\"Shape meta variables : {meta_variables.shape}\")\n",
    "\n",
    "n_features = features.shape[1]\n",
    "n_exams = features.shape[0]\n",
    "\n",
    "features = rescaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature harmonization using ComBat\n",
    "\n",
    "harmonization = CombatModel()\n",
    "\n",
    "# h_sexe, h_age = None, None\n",
    "\n",
    "h_tesla = meta_variables[:, 1].reshape(-1,1)\n",
    "h_sexe = meta_variables[:, 0].reshape(-1,1)\n",
    "h_age = meta_variables[:, 2].reshape(-1,1)\n",
    "\n",
    "features = harmonization.fit_transform(features, h_tesla, h_sexe, h_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold 100/100 - Auc train : 0.834 - Auc test: 0.938\r"
     ]
    }
   ],
   "source": [
    "# K fold experiment\n",
    "\n",
    "n_selected_features = 10\n",
    "n_splits = 10\n",
    "n_repeats = 10\n",
    "\n",
    "classifier = LogisticRegression(penalty='l2', solver='liblinear', C=0.5)\n",
    "# classifier = LogisticRegression(penalty='l1', solver='saga', C=1, l1_ratio=0)\n",
    "# classifier = DecisionTreeClassifier()\n",
    "# classifier = SVC(C=1, probability=True, kernel='rbf')\n",
    "# classifier = RandomForestClassifier(n_estimators=20, verbose=0)\n",
    "\n",
    "\n",
    "#iterator = KFold(n_splits=n_splits, random_state=None, shuffle=True)\n",
    "iterator = RepeatedKFold(n_splits=n_splits, n_repeats=10, random_state=None)\n",
    "\n",
    "features_summary = dict(zip([i for i in range(n_features)], np.zeros(n_features)))\n",
    "\n",
    "ls_auc_train = []\n",
    "ls_auc_test = []\n",
    "\n",
    "n = 0\n",
    "for train_id, test_id in iterator.split(features):\n",
    "    n += 1\n",
    "    \n",
    "    # Spliting between testing and training set\n",
    "    train_features = features[train_id]\n",
    "    train_labels = labels[train_id]\n",
    "    \n",
    "    test_labels = labels[test_id]\n",
    "    if sum(test_labels) == 0 or sum(test_labels) == len(test_labels):\n",
    "        print(\"Skipping this batch\", end='\\r')\n",
    "        continue\n",
    "    \n",
    "    # Performing feature selection using RFE or p-value (on train set only)\n",
    "    \n",
    "    ###\n",
    "    if False:\n",
    "        feature_significance_p = feature_t_test(train_features, train_labels, id_to_feat)\n",
    "        feature_significance_auc = feature_auc(train_features, train_labels, id_to_feat, LogisticRegression(penalty='none'))\n",
    "\n",
    "        selection_p_value = choose_features_from_dict(feature_significance_p, n_selected_features, feat_to_id)\n",
    "        \n",
    "        selected_features_id = selection_p_value\n",
    "    \n",
    "    ###\n",
    "    if True:\n",
    "        estimator = classifier\n",
    "        selector = RFE(estimator, n_selected_features, step=1).fit(train_features, train_labels)\n",
    "        selection_rfe = np.where(selector.support_.astype(int) > 0)[0]\n",
    "        \n",
    "        selected_features_id = selection_rfe\n",
    "        \n",
    "        for i in selected_features_id:\n",
    "            features_summary[i] += 1\n",
    "    ###\n",
    "    \n",
    "    if False:\n",
    "        agglo = FeatureAgglomeration(n_clusters=n_selected_features).fit(train_features)\n",
    "        features_s = agglo.transform(features)\n",
    "    \n",
    "    # Applying feature selection to train and test subsets\n",
    "    features_s = features[:, selected_features_id]\n",
    "    \n",
    "    # Evaluation of the classifier using LOO strategy\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    loo_predictions = []\n",
    "    loo_labels = []\n",
    "\n",
    "    X = features_s\n",
    "    y = labels\n",
    "    \n",
    "    # clf = classifier\n",
    "    clf =SVC(C=0.8, probability=True, kernel='linear')\n",
    "    \n",
    "    for in_id, out_id in loo.split(X):\n",
    "\n",
    "        X_train, X_test = X[in_id], X[out_id]\n",
    "        y_train, y_test = y[in_id], y[out_id]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        pred = clf.predict_proba(X_test)[0, 1]\n",
    "\n",
    "        loo_predictions.append(pred)\n",
    "        loo_labels.append(y_test)\n",
    "        \n",
    "    loo_predictions = np.array(loo_predictions)\n",
    "    loo_labels = np.array(loo_labels)\n",
    "    \n",
    "    # Computing metrics on train and test set, separatly\n",
    "    ls_auc_train.append(metrics.roc_auc_score(loo_labels[train_id], loo_predictions[train_id]))\n",
    "    ls_auc_test.append(metrics.roc_auc_score(loo_labels[test_id], loo_predictions[test_id]))\n",
    "    \n",
    "    print(\"K-fold {:2}/{:2} - Auc train : {:.3f} - Auc test: {:.3f}\"\n",
    "          .format(n, n_splits*n_repeats, ls_auc_train[-1], ls_auc_test[-1]) , end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training AUC : 0.790\n",
      "Average testing AUC : 0.485\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training AUC : {:.3f}\".format(np.mean(ls_auc_train)))\n",
    "print(\"Average testing AUC : {:.3f}\".format(np.mean(ls_auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> t1_OriginalGrayLevelCo-occurrenceMatrixS -  84/100\n",
      "> t1_OriginalShapeCompactness2             -  76/100\n",
      "> diff_OriginalGrayLevelDependenceMatrixLa -  64/100\n",
      "> t1_OriginalGrayLevelSizeZoneMatrixGrayLe -  44/100\n",
      "> t1_OriginalGrayLevelDependenceMatrixLarg -  44/100\n",
      "> t2_OriginalFirstOrderMinimum             -  39/100\n",
      "> t1_OriginalShapeMaximum2DDiameterColumn  -  34/100\n",
      "> gado_OriginalGrayLevelDependenceMatrixLa -  31/100\n",
      "> diff_OriginalFirstOrderMedian            -  31/100\n",
      "> t2_OriginalShapeSurfaceAreatoVolumeRatio -  26/100\n",
      "> t1_OriginalShapeMajorAxis                -  23/100\n",
      "> diff_OriginalGrayLevelSizeZoneMatrixSize -  23/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixRunEn -  22/100\n",
      "> gado_OriginalGrayLevelSizeZoneMatrixSmal -  22/100\n",
      "> gado_OriginalShapeElongation             -  21/100\n",
      "> t1_OriginalNeighboringGrayToneDifference -  19/100\n",
      "> diff_OriginalShapeLeastAxis              -  16/100\n",
      "> t1_OriginalGrayLevelSizeZoneMatrixLargeA -  15/100\n",
      "> t2_OriginalFirstOrderTotalEnergy         -  15/100\n",
      "> t2_OriginalFirstOrder90thPercentile      -  15/100\n",
      "> diff_OriginalFirstOrderTotalEnergy       -  15/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -  13/100\n",
      "> diff_OriginalNeighboringGrayToneDifferen -  13/100\n",
      "> t2_OriginalGrayLevelCo-occurrenceMatrixC -  11/100\n",
      "> gado_OriginalFirstOrderMedian            -  11/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -  11/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixC -   9/100\n",
      "> diff_OriginalShapeElongation             -   9/100\n",
      "> diff_OriginalFirstOrderVariance          -   9/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixC -   8/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixRunVa -   8/100\n",
      "> diff_OriginalGrayLevelRunLengthMatrixLon -   8/100\n",
      "> t1_OriginalShapeMaximum3DDiameter        -   7/100\n",
      "> t1_OriginalFirstOrderMaximum             -   7/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixGrayL -   7/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixRunPe -   7/100\n",
      "> t2_OriginalFirstOrderMean                -   7/100\n",
      "> t2_OriginalGrayLevelSizeZoneMatrixSmallA -   7/100\n",
      "> diff_OriginalShapeFlatness               -   7/100\n",
      "> t1_OriginalShapeElongation               -   6/100\n",
      "> t1_OriginalFirstOrder10thPercentile      -   6/100\n",
      "> t2_OriginalGrayLevelDependenceMatrixGray -   6/100\n",
      "> t1_OriginalFirstOrderEnergy              -   5/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixI -   5/100\n",
      "> t2_OriginalFirstOrderEnergy              -   5/100\n",
      "> t2_OriginalFirstOrderRootMeanSquared     -   5/100\n",
      "> t2_OriginalFirstOrderVariance            -   5/100\n",
      "> t2_OriginalGrayLevelCo-occurrenceMatrixC -   5/100\n",
      "> diff_OriginalGrayLevelRunLengthMatrixRun -   5/100\n",
      "> diff_OriginalGrayLevelSizeZoneMatrixLowG -   5/100\n",
      "> diff_OriginalGrayLevelSizeZoneMatrixLarg -   5/100\n",
      "> t1_OriginalFirstOrderMinimum             -   4/100\n",
      "> t1_OriginalGrayLevelDependenceMatrixLowG -   4/100\n",
      "> t2_OriginalFirstOrderRange               -   4/100\n",
      "> t2_OriginalGrayLevelCo-occurrenceMatrixC -   4/100\n",
      "> gado_OriginalShapeMinorAxis              -   4/100\n",
      "> diff_OriginalFirstOrderRobustMeanAbsolut -   4/100\n",
      "> t1_OriginalShapeMaximum2DDiameterRow     -   3/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixA -   3/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixJ -   3/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixShort -   3/100\n",
      "> diff_OriginalFirstOrderMinimum           -   3/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   3/100\n",
      "> t1_OriginalShapeVolume                   -   2/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixC -   2/100\n",
      "> t1_OriginalNeighboringGrayToneDifference -   2/100\n",
      "> t2_OriginalShapeFlatness                 -   2/100\n",
      "> t2_OriginalFirstOrderMedian              -   2/100\n",
      "> t2_OriginalGrayLevelRunLengthMatrixLongR -   2/100\n",
      "> t2_OriginalNeighboringGrayToneDifference -   2/100\n",
      "> gado_OriginalShapeCompactness2           -   2/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -   2/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -   2/100\n",
      "> diff_OriginalFirstOrderInterquartileRang -   2/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   2/100\n",
      "> diff_OriginalGrayLevelSizeZoneMatrixSmal -   2/100\n",
      "> t1_OriginalShapeCompactness1             -   1/100\n",
      "> t1_OriginalShapeLeastAxis                -   1/100\n",
      "> t1_OriginalFirstOrderMean                -   1/100\n",
      "> t1_OriginalFirstOrderRange               -   1/100\n",
      "> t1_OriginalFirstOrderKurtosis            -   1/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixD -   1/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixI -   1/100\n",
      "> t1_OriginalGrayLevelCo-occurrenceMatrixS -   1/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixLowGr -   1/100\n",
      "> t1_OriginalGrayLevelRunLengthMatrixShort -   1/100\n",
      "> t1_OriginalNeighboringGrayToneDifference -   1/100\n",
      "> t2_OriginalGrayLevelCo-occurrenceMatrixI -   1/100\n",
      "> t2_OriginalGrayLevelSizeZoneMatrixZoneEn -   1/100\n",
      "> t2_OriginalGrayLevelSizeZoneMatrixLargeA -   1/100\n",
      "> t2_OriginalNeighboringGrayToneDifference -   1/100\n",
      "> gado_OriginalShapeCompactness1           -   1/100\n",
      "> gado_OriginalShapeFlatness               -   1/100\n",
      "> gado_OriginalFirstOrderMinimum           -   1/100\n",
      "> gado_OriginalFirstOrderSkewness          -   1/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> gado_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> gado_OriginalGrayLevelRunLengthMatrixSho -   1/100\n",
      "> gado_OriginalGrayLevelSizeZoneMatrixSmal -   1/100\n",
      "> gado_OriginalGrayLevelDependenceMatrixSm -   1/100\n",
      "> diff_OriginalShapeSurfaceAreatoVolumeRat -   1/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> diff_OriginalGrayLevelCo-occurrenceMatri -   1/100\n",
      "> diff_OriginalGrayLevelDependenceMatrixLo -   1/100\n"
     ]
    }
   ],
   "source": [
    "for i in order_dict(features_summary).keys():\n",
    "    if features_summary[i] > 0:\n",
    "        print(\">{:40.40} - {:3}/100\".format(id_to_feat[i], int(features_summary[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "------ With student test\n",
    "--> Harmonization with tesla information only\n",
    "Average training AUC : 0.656\n",
    "Average testing AUC : 0.541\n",
    "\n",
    "--> Harmonization with tesla, sexe and age information\n",
    "Average training AUC : 0.658\n",
    "Average testing AUC : 0.568\n",
    "\n",
    "--> Using only t1 information \n",
    "Average training AUC : 0.661\n",
    "Average testing AUC : 0.621\n",
    "\n",
    "--> Using only t2 information \n",
    "Average training AUC : 0.603\n",
    "Average testing AUC : 0.555\n",
    "\n",
    "--> Using only gado information \n",
    "Average training AUC : 0.581\n",
    "Average testing AUC : 0.585\n",
    "\n",
    "--> Using only diff information \n",
    "Average training AUC : 0.623\n",
    "Average testing AUC : 0.597\n",
    "\n",
    "--> Using only t1, diff information \n",
    "Average training AUC : 0.658\n",
    "Average testing AUC : 0.570\n",
    "\n",
    "\n",
    "----- With RFE\n",
    "--> Using all modalities\n",
    "Average training AUC : 0.784\n",
    "Average testing AUC : 0.477\n",
    "\n",
    "--> Using all modalities and C=0.5\n",
    "Average training AUC : 0.753\n",
    "Average testing AUC : 0.502\n",
    "\n",
    "--> Using only t1 information \n",
    "Average training AUC : 0.737\n",
    "Average testing AUC : 0.591\n",
    "\n",
    "--> Using only t1 information and C=0.5\n",
    "Average training AUC : 0.699\n",
    "Average testing AUC : 0.624\n",
    "\n",
    "--> Using only t1 information and C=0.4\n",
    "Average training AUC : 0.684\n",
    "Average testing AUC : 0.627\n",
    "\n",
    "--> Using only t1 information and C=0.5 for feature selection and SVC (linear) C=1 for predictions \n",
    "Average training AUC : 0.742\n",
    "Average testing AUC : 0.638\n",
    "\n",
    "--> Using only t1 information and SVC (linear) C=0.5 for feature selection and SVC (linear) C=0.5 for predictions \n",
    "Average training AUC : 0.757\n",
    "Average testing AUC : 0.624\n",
    "\n",
    "\n",
    "\n",
    "----- With Feature agglomeration\n",
    "--> Using all modalities\n",
    "Average training AUC : 0.632\n",
    "Average testing AUC : 0.642\n",
    "\n",
    "--> Using all modalities and 5 selected features\n",
    "Average training AUC : 0.575\n",
    "Average testing AUC : 0.576\n",
    "\n",
    "--> Using all modalities and 20 selected features\n",
    "Average training AUC : 0.607\n",
    "Average testing AUC : 0.601\n",
    "\n",
    "--> Using all modalities and SVC (rbf)\n",
    "Average training AUC : 0.604\n",
    "Average testing AUC : 0.602\n",
    "\n",
    "--> Using all modalities and SVC (rbf) and 20 selected features\n",
    "Average training AUC : 0.586\n",
    "Average testing AUC : 0.606\n",
    "\n",
    "--> Using only t1 information \n",
    "Average training AUC : 0.627\n",
    "Average testing AUC : 0.620\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
